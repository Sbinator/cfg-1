Automatically generated by Mendeley Desktop 1.17.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Stanley2009,
abstract = {Research in neuroevolution-that is, evolving artificial neural networks (ANNs) through evolutionary algorithms-is inspired by the evolution of biological brains, which can contain trillions of connections. Yet while neuroevolution has produced successful results, the scale of natural brains remains far beyond reach. This article presents a method called hypercube-based NeuroEvolution of Augmenting Topologies (HyperNEAT) that aims to narrow this gap. HyperNEAT employs an indirect encoding called connective compositional pattern-producing networks (CPPNs) that can produce connectivity patterns with symmetries and repeating motifs by interpreting spatial patterns generated within a hypercube as connectivity patterns in a lower-dimensional space. This approach can exploit the geometry of the task by mapping its regularities onto the topology of the network, thereby shifting problem difficulty away from dimensionality to the underlying problem structure. Furthermore, connective CPPNs can represent the same connectivity pattern at any resolution, allowing ANNs to scale to new numbers of inputs and outputs without further evolution. HyperNEAT is demonstrated through visual discrimination and food-gathering tasks, including successful visual discrimination networks containing over eight million connections. The main conclusion is that the ability to explore the space of regular connectivity patterns opens up a new class of complex high-dimensional tasks to neuroevolution.},
annote = {The original HyperNEAT paper.},
author = {Stanley, Kenneth O and D'Ambrosio, David B and Gauci, Jason},
doi = {10.1162/artl.2009.15.2.15202},
file = {:media/lepisma/Data/Cloud/Papers/Stanley, D'Ambrosio, Gauci/Stanley, D'Ambrosio, Gauci - A hypercube-based encoding for evolving large-scale neural networks.pdf:pdf},
isbn = {1064-5462 (Print)$\backslash$r1064-5462 (Linking)},
issn = {1064-5462},
journal = {Artificial Life},
keywords = {Biological Evolution,Computational Biology,Food,HyperNEAT,Nerve Net,Nerve Net: metabolism,Visual Perception,Visual Perception: physiology},
number = {2},
pages = {185--212},
pmid = {19199382},
title = {{A hypercube-based encoding for evolving large-scale neural networks.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19199382},
volume = {15},
year = {2009}
}
@article{Fernando2016,
abstract = {Data represented as strings abounds in biology, linguistics, document mining, web search and many other fields. Such data often have a hierarchical structure, either because they were artificially designed and composed in a hierarchical manner or because there is an underlying evolutionary process that creates repeatedly more complex strings from simpler substrings. We propose a framework, referred to as "Lexis", that produces an optimized hierarchical representation of a given set of "target" strings. The resulting hierarchy, "Lexis-DAG", shows how to construct each target through the concatenation of intermediate substrings, minimizing the total number of such concatenations or DAG edges. The Lexis optimization problem is related to the smallest grammar problem. After we prove its NP-Hardness for two cost formulations, we propose an efficient greedy algorithm for the construction of Lexis-DAGs. We also consider the problem of identifying the set of intermediate nodes (substrings) that collectively form the "core" of a Lexis-DAG, which is important in the analysis of Lexis-DAGs. We show that the Lexis framework can be applied in diverse applications such as optimized synthesis of DNA fragments in genomic libraries, hierarchical structure discovery in protein sequences, dictionary-based text compression, and feature extraction from a set of documents.},
archivePrefix = {arXiv},
arxivId = {arXiv:1602.05561v1},
author = {Fernando, Chrisantha and Banarse, Dylan and Reynolds, Malcolm and Besse, Frederic and Pfau, David and Jaderberg, Max and Lanctot, Marc and Wierstra, Daan},
doi = {10.1145/2908812.2908890},
eprint = {arXiv:1602.05561v1},
file = {:media/lepisma/Data/Cloud/Papers/Fernando et al/Fernando et al. - Convolution by Evolution.pdf:pdf},
isbn = {9781450342063},
journal = {Proceedings of the 2016 on Genetic and Evolutionary Computation Conference - GECCO '16},
keywords = {compositional pattern producing networks,cppns,de-,mnist,noising autoencoder},
pages = {109--116},
title = {{Convolution by Evolution}},
year = {2016}
}
@article{Ha2016,
abstract = {This work explores hypernetworks: an approach of using a small network, also known as a hypernetwork, to generate the weights for a larger network. Hypernetworks provide an abstraction that is similar to what is found in nature: the relationship between a genotype - the hypernetwork - and a phenotype - the main network. Though they are also reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to-end with backpropagation and thus are usually faster. The focus of this work is to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as relaxed form of weight-sharing across layers. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-art results on a variety of language modeling tasks with Character-Level Penn Treebank and Hutter Prize Wikipedia datasets, challenging the weight-sharing paradigm for recurrent networks. Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters.},
archivePrefix = {arXiv},
arxivId = {1609.09106},
author = {Ha, David and Dai, Andrew and Le, Quoc V.},
eprint = {1609.09106},
file = {:media/lepisma/Data/Cloud/Papers/Ha, Dai, Le/Ha, Dai, Le - HyperNetworks.pdf:pdf},
title = {{HyperNetworks}},
url = {http://arxiv.org/abs/1609.09106},
year = {2016}
}
